<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>haystackapi.providers.import_s3 API documentation</title>
<meta name="description" content="Import haystack file in S3 bucket. Manage the versions of files." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>haystackapi.providers.import_s3</code></h1>
</header>
<section id="section-intro">
<p>Import haystack file in S3 bucket. Manage the versions of files.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
# Import data in S3 bucket
# See the accompanying LICENSE file.
# (C) 2021 Engie Digital
#
# vim: set ts=4 sts=4 et tw=78 sw=4 si:
&#34;&#34;&#34;
Import haystack file in S3 bucket. Manage the versions of files.
&#34;&#34;&#34;
import base64
import gzip
import logging
import os
import sys
import urllib
from hashlib import md5
from io import BytesIO
from multiprocessing.dummy import freeze_support
from multiprocessing.pool import ThreadPool
from pathlib import Path
from threading import Lock
from typing import Tuple
from urllib.parse import ParseResult, urlparse

import boto3
import click
from botocore.exceptions import ClientError

from ..dumper import dump
from ..grid import Grid
from ..parser import parse, suffix_to_mode, EmptyGrid
from ..zincparser import ZincParseException

log = logging.getLogger(&#34;import_s3&#34;)

VERIFY = True  # See https://tinyurl.com/y5tap6ys
POOL_SIZE = 20

lock = Lock()


def _download_uri(parsed_uri: ParseResult) -&gt; bytes:
    &#34;&#34;&#34; Return decompressed data from url &#34;&#34;&#34;
    if parsed_uri.scheme == &#34;s3&#34;:
        s3_client = boto3.client(
            &#34;s3&#34;,
            endpoint_url=os.environ.get(&#34;AWS_S3_ENDPOINT&#34;, None),
            verify=VERIFY,  # See https://tinyurl.com/y5tap6ys
        )

        stream = BytesIO()
        s3_client.download_fileobj(parsed_uri.netloc, parsed_uri.path[1:], stream)
        data = stream.getvalue()
    else:
        # Manage default cwd
        uri = parsed_uri.geturl()
        if not parsed_uri.scheme:
            uri = Path.cwd().joinpath(uri).as_uri()
        with urllib.request.urlopen(uri) as response:
            data = response.read()
    return data


def _get_hash_of_s3_file(s3_client,
                         parsed):
    try:
        head = s3_client.head_object(Bucket=parsed.hostname,
                                     Key=parsed.path[1:],
                                     )
        return head[&#34;ETag&#34;][1:-1]
    except ClientError as ex:
        if ex.response[&#39;Error&#39;][&#39;Code&#39;] == &#39;NoSuchKey&#39;:
            # Target key not found
            return &#39;&#39;
        raise


def merge_timeseries(source_grid: Grid,
                     destination_grid: Grid,
                     mode: str,
                     compress: bool) -&gt; Tuple[bytes, str]:
    &#34;&#34;&#34; Merge time series.
        If the destination time series has older values, insert these values at the beginning
        of the current time series.
        It is not to forget values
    &#34;&#34;&#34;
    assert &#39;ts&#39; in source_grid.column, &#34;The source grid must have ts,value columns&#34;
    if &#39;ts&#39; in destination_grid.column:
        if id(destination_grid) != id(source_grid):
            destination_grid.sort(&#39;ts&#39;)
            source_grid.sort(&#39;ts&#39;)
            start_source = source_grid[0][&#39;ts&#39;]
            source_grid.extend(filter(lambda row: row[&#39;ts&#39;] &lt; start_source, destination_grid))
            source_grid.sort(&#39;ts&#39;)

    source_data = dump(source_grid, mode).encode(&#39;UTF8&#39;)
    if compress:
        source_data = gzip.compress(source_data)
    md5_digest = md5(source_data)
    b64_digest = base64.b64encode(md5_digest.digest()).decode(&#34;UTF8&#34;)
    return source_data, b64_digest


def update_grid_on_s3(parsed_source: ParseResult,  # pylint: disable=too-many-locals
                      parsed_destination: ParseResult,
                      compare_grid: bool,
                      time_series: bool,
                      force: bool,
                      merge_ts: bool,
                      use_thread: bool = True
                      ) -&gt; None:
    log.debug(&#34;update %s&#34;, (parsed_source.geturl(),))
    s3_client = boto3.client(
        &#34;s3&#34;,
        endpoint_url=os.environ.get(&#34;AWS_S3_ENDPOINT&#34;, None),
    )
    suffix = Path(parsed_source.path).suffix
    use_gzip = False
    if parsed_source.scheme == &#39;s3&#39; and not compare_grid:
        if time_series:
            source_data = _download_uri(parsed_source)
            md5_digest = md5(source_data)
            source_etag = &#39;&#39;
            if not force:
                source_etag = md5_digest.hexdigest()
        else:
            source_etag = _get_hash_of_s3_file(s3_client, parsed_source)
        target_etag = _get_hash_of_s3_file(s3_client, parsed_destination)

        if force or source_etag != target_etag:
            s3_client.copy_object(Bucket=parsed_destination.hostname,
                                  Key=parsed_destination.path[1:],
                                  CopySource={&#39;Bucket&#39;: parsed_source.hostname,
                                              &#39;Key&#39;: parsed_source.path[1:]
                                              },
                                  )
            log.info(&#34;%s updated (copy between s3 bucket)&#34;, parsed_source.geturl())
        else:
            log.debug(&#34;%s not modified (same grid)&#34;, parsed_source.geturl())
    else:
        source_data = _download_uri(parsed_source)
        md5_digest = md5(source_data)
        b64_digest = base64.b64encode(md5_digest.digest()).decode(&#34;UTF8&#34;)
        source_etag = &#39;&#39;
        source_grid = None
        if not force:
            source_etag = md5_digest.hexdigest()
        if time_series or compare_grid or merge_ts:
            unzipped_source_data = source_data
            if suffix == &#34;.gz&#34;:
                use_gzip = True
                unzipped_source_data = gzip.decompress(source_data)
                suffix = Path(parsed_source.path).suffixes[-2]

            try:
                with lock:
                    source_grid = parse(unzipped_source_data.decode(&#34;utf-8-sig&#34;),
                                        suffix_to_mode(suffix))
                path = parsed_destination.path[1:]
                destination_data = s3_client.get_object(Bucket=parsed_destination.hostname,
                                                        Key=path,
                                                        IfNoneMatch=source_etag)[&#39;Body&#39;].read()
                if parsed_source.path.endswith(&#34;.gz&#34;):
                    destination_data = gzip.decompress(destination_data)
                with lock:
                    destination_grid = parse(destination_data.decode(&#34;utf-8-sig&#34;),
                                             suffix_to_mode(suffix))

            except ClientError as ex:
                if ex.response[&#39;Error&#39;][&#39;Code&#39;] == &#39;NoSuchKey&#39;:
                    # Target key not found
                    destination_grid = EmptyGrid
                elif ex.response[&#39;Error&#39;][&#39;Code&#39;] == &#39;304&#39;:
                    # Target not modified
                    if source_grid:
                        destination_grid = source_grid
                    else:
                        raise
                else:
                    raise
            except ZincParseException as ex:
                # Ignore. Override target
                log.warning(&#34;Zinc parser exception with %s&#34;, (parsed_destination.geturl()))
                destination_grid = EmptyGrid

        if force or not compare_grid or (destination_grid - source_grid):
            if not force and merge_ts:  # PPR: if TS, limit the number of AWS versions ?
                source_data, b64_digest = merge_timeseries(source_grid,
                                                           destination_grid,
                                                           suffix_to_mode(suffix), use_gzip)
            path = parsed_destination.path[1:]
            s3_client.put_object(Body=source_data,
                                 Bucket=parsed_destination.hostname,
                                 Key=path,
                                 ContentMD5=b64_digest
                                 )
            log.info(&#34;%s updated (put in s3 bucket)&#34;, parsed_source.geturl())
        else:
            log.debug(&#34;%s not modified (same grid)&#34;, parsed_source.geturl())

    # Now, it&#39;s time to upload the referenced time-series
    if time_series:
        source_url = parsed_source.geturl()
        source_home = source_url[0:source_url.rfind(&#39;/&#39;) + 1]
        destination_url = parsed_destination.geturl()
        destination_home = destination_url[0:destination_url.rfind(&#39;/&#39;) + 1]
        requests = []
        for row in source_grid:
            if &#34;hisURI&#34; in row:
                source_time_serie = source_home + row[&#34;hisURI&#34;]
                destination_time_serie = destination_home + row[&#34;hisURI&#34;]
                if use_thread:
                    requests.append((
                        urlparse(source_time_serie),
                        urlparse(destination_time_serie),
                        False,
                        False,
                        force,
                        True))
                else:
                    update_grid_on_s3(
                        urlparse(source_time_serie),
                        urlparse(destination_time_serie),
                        compare_grid=False,
                        time_series=False,
                        force=force,
                        merge_ts=True)
        if requests:
            with ThreadPool(processes=POOL_SIZE) as pool:
                pool.starmap(update_grid_on_s3, requests)


def import_in_s3(source: str,
                 destination: str,
                 compare: bool = False,
                 time_series: bool = True,
                 force: bool = False):
    &#34;&#34;&#34;
    Import source grid file on destination, only if something was changed.
    The source can use all kind of URL or relative path.
    The destination must be an s3 URL. If this URL end with &#39;/&#39; the source filename was used.
    &#34;&#34;&#34;
    parsed_source = urlparse(source)
    parsed_destination = urlparse(destination)
    if not parsed_destination.path:
        destination = destination + &#34;/&#34;
    if destination.endswith(&#39;/&#39;):
        destination += Path(parsed_source.path).name
    parsed_destination = urlparse(destination)

    update_grid_on_s3(parsed_source,
                      parsed_destination,
                      compare,
                      time_series,
                      force,
                      merge_ts=False
                      )


def aws_handler(event, context):
    &#34;&#34;&#34;
    AWS Handler.
    Set the environment variable HAYSTACK_URL and HAYSTACK_DB
    &#34;&#34;&#34;
    hs_source_url = os.environ.get(&#34;HAYSTACK_SOURCE_URL&#34;)
    hs_target_url = os.environ.get(&#34;HAYSTACK_URL&#34;)
    assert hs_source_url, &#34;Set `HAYSTACK_SOURCE_URL`&#34;
    assert hs_target_url, &#34;Set `HAYSTACK_URL`&#34;
    import_in_s3(hs_source_url, hs_target_url)


@click.command(short_help=&#39;Import haystack file in database&#39;)
@click.argument(&#39;source_url&#39;,
                metavar=&#39;&lt;haystack url&gt;&#39;,
                # help=&#39;filename or url (may be s3:/...)&#39;
                )
@click.argument(&#39;target_url&#39;,
                metavar=&#39;&lt;s3 target url&gt;&#39;,
                # help=&#39;s3 url&#39;
                )
@click.option(&#34;--compare/--no-compare&#34;,
              help=&#39;Compare grid before upload datas&#39;,
              default=True
              )
@click.option(&#34;--time-series/--no-time-series&#34;,
              help=&#39;Import time-series referenced with hisURI tag&#39;,
              default=True
              )
@click.option(&#34;--force/--no-force&#34;,
              help=&#39;Force to upload data without verifications or update&#39;,
              default=False
              )
def main(source_url: str, target_url: str, compare: bool, time_series: bool, force: bool) -&gt; int:
    &#34;&#34;&#34;
    Import haystack file for file or URL, to s3 bucket.
    Only the difference was imported, with a new version of ontology.
    If the destination time series exists, and old values are present, they are recovered
    at the beginning of the grid.
    &#34;&#34;&#34;
    try:
        import_in_s3(source_url, target_url, compare, time_series, force)
        print(f&#34;{source_url} imported in {target_url}&#34;)
    except ClientError as ex:
        if ex.response[&#39;Error&#39;][&#39;Code&#39;] == &#39;ExpiredToken&#39;:
            logging.error(&#34;Refresh the token.&#34;)
        else:
            raise

    return 0


if __name__ == &#39;__main__&#39;:
    freeze_support()
    logging.basicConfig(level=os.environ.get(&#34;LOG_LEVEL&#34;, &#34;ERROR&#34;))
    sys.exit(main())  # pylint: disable=no-value-for-parameter</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="haystackapi.providers.import_s3.aws_handler"><code class="name flex">
<span>def <span class="ident">aws_handler</span></span>(<span>event, context)</span>
</code></dt>
<dd>
<div class="desc"><p>AWS Handler.
Set the environment variable HAYSTACK_URL and HAYSTACK_DB</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def aws_handler(event, context):
    &#34;&#34;&#34;
    AWS Handler.
    Set the environment variable HAYSTACK_URL and HAYSTACK_DB
    &#34;&#34;&#34;
    hs_source_url = os.environ.get(&#34;HAYSTACK_SOURCE_URL&#34;)
    hs_target_url = os.environ.get(&#34;HAYSTACK_URL&#34;)
    assert hs_source_url, &#34;Set `HAYSTACK_SOURCE_URL`&#34;
    assert hs_target_url, &#34;Set `HAYSTACK_URL`&#34;
    import_in_s3(hs_source_url, hs_target_url)</code></pre>
</details>
</dd>
<dt id="haystackapi.providers.import_s3.import_in_s3"><code class="name flex">
<span>def <span class="ident">import_in_s3</span></span>(<span>source: str, destination: str, compare: bool = False, time_series: bool = True, force: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Import source grid file on destination, only if something was changed.
The source can use all kind of URL or relative path.
The destination must be an s3 URL. If this URL end with '/' the source filename was used.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def import_in_s3(source: str,
                 destination: str,
                 compare: bool = False,
                 time_series: bool = True,
                 force: bool = False):
    &#34;&#34;&#34;
    Import source grid file on destination, only if something was changed.
    The source can use all kind of URL or relative path.
    The destination must be an s3 URL. If this URL end with &#39;/&#39; the source filename was used.
    &#34;&#34;&#34;
    parsed_source = urlparse(source)
    parsed_destination = urlparse(destination)
    if not parsed_destination.path:
        destination = destination + &#34;/&#34;
    if destination.endswith(&#39;/&#39;):
        destination += Path(parsed_source.path).name
    parsed_destination = urlparse(destination)

    update_grid_on_s3(parsed_source,
                      parsed_destination,
                      compare,
                      time_series,
                      force,
                      merge_ts=False
                      )</code></pre>
</details>
</dd>
<dt id="haystackapi.providers.import_s3.merge_timeseries"><code class="name flex">
<span>def <span class="ident">merge_timeseries</span></span>(<span>source_grid: haystackapi.grid.Grid, destination_grid: haystackapi.grid.Grid, mode: str, compress: bool) ‑> Tuple[bytes, str]</span>
</code></dt>
<dd>
<div class="desc"><p>Merge time series.
If the destination time series has older values, insert these values at the beginning
of the current time series.
It is not to forget values</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_timeseries(source_grid: Grid,
                     destination_grid: Grid,
                     mode: str,
                     compress: bool) -&gt; Tuple[bytes, str]:
    &#34;&#34;&#34; Merge time series.
        If the destination time series has older values, insert these values at the beginning
        of the current time series.
        It is not to forget values
    &#34;&#34;&#34;
    assert &#39;ts&#39; in source_grid.column, &#34;The source grid must have ts,value columns&#34;
    if &#39;ts&#39; in destination_grid.column:
        if id(destination_grid) != id(source_grid):
            destination_grid.sort(&#39;ts&#39;)
            source_grid.sort(&#39;ts&#39;)
            start_source = source_grid[0][&#39;ts&#39;]
            source_grid.extend(filter(lambda row: row[&#39;ts&#39;] &lt; start_source, destination_grid))
            source_grid.sort(&#39;ts&#39;)

    source_data = dump(source_grid, mode).encode(&#39;UTF8&#39;)
    if compress:
        source_data = gzip.compress(source_data)
    md5_digest = md5(source_data)
    b64_digest = base64.b64encode(md5_digest.digest()).decode(&#34;UTF8&#34;)
    return source_data, b64_digest</code></pre>
</details>
</dd>
<dt id="haystackapi.providers.import_s3.update_grid_on_s3"><code class="name flex">
<span>def <span class="ident">update_grid_on_s3</span></span>(<span>parsed_source: urllib.parse.ParseResult, parsed_destination: urllib.parse.ParseResult, compare_grid: bool, time_series: bool, force: bool, merge_ts: bool, use_thread: bool = True) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_grid_on_s3(parsed_source: ParseResult,  # pylint: disable=too-many-locals
                      parsed_destination: ParseResult,
                      compare_grid: bool,
                      time_series: bool,
                      force: bool,
                      merge_ts: bool,
                      use_thread: bool = True
                      ) -&gt; None:
    log.debug(&#34;update %s&#34;, (parsed_source.geturl(),))
    s3_client = boto3.client(
        &#34;s3&#34;,
        endpoint_url=os.environ.get(&#34;AWS_S3_ENDPOINT&#34;, None),
    )
    suffix = Path(parsed_source.path).suffix
    use_gzip = False
    if parsed_source.scheme == &#39;s3&#39; and not compare_grid:
        if time_series:
            source_data = _download_uri(parsed_source)
            md5_digest = md5(source_data)
            source_etag = &#39;&#39;
            if not force:
                source_etag = md5_digest.hexdigest()
        else:
            source_etag = _get_hash_of_s3_file(s3_client, parsed_source)
        target_etag = _get_hash_of_s3_file(s3_client, parsed_destination)

        if force or source_etag != target_etag:
            s3_client.copy_object(Bucket=parsed_destination.hostname,
                                  Key=parsed_destination.path[1:],
                                  CopySource={&#39;Bucket&#39;: parsed_source.hostname,
                                              &#39;Key&#39;: parsed_source.path[1:]
                                              },
                                  )
            log.info(&#34;%s updated (copy between s3 bucket)&#34;, parsed_source.geturl())
        else:
            log.debug(&#34;%s not modified (same grid)&#34;, parsed_source.geturl())
    else:
        source_data = _download_uri(parsed_source)
        md5_digest = md5(source_data)
        b64_digest = base64.b64encode(md5_digest.digest()).decode(&#34;UTF8&#34;)
        source_etag = &#39;&#39;
        source_grid = None
        if not force:
            source_etag = md5_digest.hexdigest()
        if time_series or compare_grid or merge_ts:
            unzipped_source_data = source_data
            if suffix == &#34;.gz&#34;:
                use_gzip = True
                unzipped_source_data = gzip.decompress(source_data)
                suffix = Path(parsed_source.path).suffixes[-2]

            try:
                with lock:
                    source_grid = parse(unzipped_source_data.decode(&#34;utf-8-sig&#34;),
                                        suffix_to_mode(suffix))
                path = parsed_destination.path[1:]
                destination_data = s3_client.get_object(Bucket=parsed_destination.hostname,
                                                        Key=path,
                                                        IfNoneMatch=source_etag)[&#39;Body&#39;].read()
                if parsed_source.path.endswith(&#34;.gz&#34;):
                    destination_data = gzip.decompress(destination_data)
                with lock:
                    destination_grid = parse(destination_data.decode(&#34;utf-8-sig&#34;),
                                             suffix_to_mode(suffix))

            except ClientError as ex:
                if ex.response[&#39;Error&#39;][&#39;Code&#39;] == &#39;NoSuchKey&#39;:
                    # Target key not found
                    destination_grid = EmptyGrid
                elif ex.response[&#39;Error&#39;][&#39;Code&#39;] == &#39;304&#39;:
                    # Target not modified
                    if source_grid:
                        destination_grid = source_grid
                    else:
                        raise
                else:
                    raise
            except ZincParseException as ex:
                # Ignore. Override target
                log.warning(&#34;Zinc parser exception with %s&#34;, (parsed_destination.geturl()))
                destination_grid = EmptyGrid

        if force or not compare_grid or (destination_grid - source_grid):
            if not force and merge_ts:  # PPR: if TS, limit the number of AWS versions ?
                source_data, b64_digest = merge_timeseries(source_grid,
                                                           destination_grid,
                                                           suffix_to_mode(suffix), use_gzip)
            path = parsed_destination.path[1:]
            s3_client.put_object(Body=source_data,
                                 Bucket=parsed_destination.hostname,
                                 Key=path,
                                 ContentMD5=b64_digest
                                 )
            log.info(&#34;%s updated (put in s3 bucket)&#34;, parsed_source.geturl())
        else:
            log.debug(&#34;%s not modified (same grid)&#34;, parsed_source.geturl())

    # Now, it&#39;s time to upload the referenced time-series
    if time_series:
        source_url = parsed_source.geturl()
        source_home = source_url[0:source_url.rfind(&#39;/&#39;) + 1]
        destination_url = parsed_destination.geturl()
        destination_home = destination_url[0:destination_url.rfind(&#39;/&#39;) + 1]
        requests = []
        for row in source_grid:
            if &#34;hisURI&#34; in row:
                source_time_serie = source_home + row[&#34;hisURI&#34;]
                destination_time_serie = destination_home + row[&#34;hisURI&#34;]
                if use_thread:
                    requests.append((
                        urlparse(source_time_serie),
                        urlparse(destination_time_serie),
                        False,
                        False,
                        force,
                        True))
                else:
                    update_grid_on_s3(
                        urlparse(source_time_serie),
                        urlparse(destination_time_serie),
                        compare_grid=False,
                        time_series=False,
                        force=force,
                        merge_ts=True)
        if requests:
            with ThreadPool(processes=POOL_SIZE) as pool:
                pool.starmap(update_grid_on_s3, requests)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="haystackapi.providers" href="index.html">haystackapi.providers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="haystackapi.providers.import_s3.aws_handler" href="#haystackapi.providers.import_s3.aws_handler">aws_handler</a></code></li>
<li><code><a title="haystackapi.providers.import_s3.import_in_s3" href="#haystackapi.providers.import_s3.import_in_s3">import_in_s3</a></code></li>
<li><code><a title="haystackapi.providers.import_s3.merge_timeseries" href="#haystackapi.providers.import_s3.merge_timeseries">merge_timeseries</a></code></li>
<li><code><a title="haystackapi.providers.import_s3.update_grid_on_s3" href="#haystackapi.providers.import_s3.update_grid_on_s3">update_grid_on_s3</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>